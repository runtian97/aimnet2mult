# Default configuration for mixed-fidelity AIMNet2 training
#
# This implements mixed-fidelity learning where:
# - Batches can contain molecules from different fidelities
# - Each molecule is processed with fidelity-specific embeddings via shifted atomic numbers
# - Readouts can be fidelity-dependent

# Required to be set
run_name: ???
project_name: ???

# Fidelity settings
fidelity_offset: 200              # Offset between fidelities
use_fidelity_readouts: true       # Use separate readouts per fidelity

data:
  # Fidelity datasets (specified via command line)
  fidelity_datasets: {}

  # Fidelity weights (specified via command line)
  fidelity_weights: {}

  # Optional: separate validation datasets per fidelity
  val_fidelity_datasets: null

  # SAE files (specified via command line)
  sae:
    energy:
      files: {}
      mode: linreg

  # Validation settings
  val_fraction: 0.01  # Use 1% for validation (faster for large datasets >10M samples)
  separate_val: true
  ddp_load_full_dataset: false

  # Data keys
  # Required inputs: coord, numbers, charge
  # Optional inputs: mult (multiplicity, defaults to 1 if missing)
  x: [coord, numbers, charge, mult]
  # Required labels: energy
  # Optional labels: forces, charges, spin_charges (masked if missing)
  y: [energy, forces, charges, spin_charges]

  # Sampler configuration
  samplers:
    train:
      kwargs:
        batch_size: 32
        batch_mode: atoms
        shuffle: true
        batches_per_epoch: 100
        sampling_strategy: weighted  # Still use weighted sampling

    val:
      kwargs:
        batch_size: 64
        batch_mode: atoms
        shuffle: false
        batches_per_epoch: 100  # Limit validation batches (use -1 for full dataset)
        sampling_strategy: uniform

  # DataLoader settings
  loaders:
    train:
      num_workers: 0
      pin_memory: true
    val:
      num_workers: 0
      pin_memory: true

# Loss function
loss:
  class: aimnet2mult.train.loss.MTLoss
  kwargs:
    components:
      energy:
        fn: aimnet2mult.train.loss.energy_loss_fn
        weight: 1.0
      forces:
        fn: aimnet2mult.train.loss.peratom_loss_fn
        weight: 0.2
        kwargs:
          key_true: forces
          key_pred: forces
      charges:
        fn: aimnet2mult.train.loss.peratom_loss_fn
        weight: 0.05
        kwargs:
          key_true: charges
          key_pred: charges
      spin_charges:
        fn: aimnet2mult.train.loss.peratom_loss_fn
        weight: 0.05
        kwargs:
          key_true: spin_charges
          key_pred: spin_charges

# Optimizer
optimizer:
  force_no_train: []
  force_train: []
  class: torch.optim.RAdam
  kwargs:
    lr: 0.0001
    weight_decay: 1e-8
  param_groups:
    embeddings:
      re: 'afv.weight'
      lr: 0.00005
      weight_decay: 0.0
    shifts:
      re: '.*.atomic_shift.shifts.weight$'
      weight_decay: 0.0

# Learning rate scheduler
# Set to null to disable scheduler
# Common options:
#
# 1. ReduceLROnPlateau (adaptive, reduce LR when validation loss plateaus):
# scheduler:
#   class: ignite.handlers.param_scheduler.ReduceLROnPlateauScheduler
#   kwargs:
#     metric_name: loss
#     factor: 0.75        # Multiply LR by this factor when reducing
#     patience: 10        # Number of epochs with no improvement before reducing LR
#   terminate_on_low_lr: 1.0e-5  # Stop training if LR drops below this value
#
# 2. CosineAnnealingScheduler (smooth cosine decay - recommended for fixed epochs):
# scheduler:
#   class: ignite.handlers.param_scheduler.CosineAnnealingScheduler
#   kwargs:
#     param_name: lr
#     start_value: 1.0e-4 # Starting learning rate
#     end_value: 1.0e-6   # Final learning rate
#     cycle_size: 100     # Cycle over N epochs (should match trainer.epochs)
#   terminate_on_low_lr: 0.0  # Set to 0.0 to disable early termination
#
# 3. StepLR (reduce LR at fixed intervals):
# scheduler:
#   class: torch.optim.lr_scheduler.StepLR
#   kwargs:
#     step_size: 30       # Reduce LR every N epochs
#     gamma: 0.5          # Multiply LR by this factor
#
# 4. ExponentialLR (exponential decay):
# scheduler:
#   class: torch.optim.lr_scheduler.ExponentialLR
#   kwargs:
#     gamma: 0.95         # Decay rate per epoch
#
scheduler: null

# Training
trainer:
  epochs: 10

# Checkpointing
checkpoint:
  dirname: checkpoints
  filename_prefix: ${run_name}
  kwargs:
    n_saved: 2
    require_empty: false

# Weights & Biases logger
wandb:
  init:
    name: ${run_name}
    mode: online
    entity: null
    project: ${project_name}
    notes: null
  watch_model:
    log: all
    log_freq: 1000
    log_graph: true
  # Logging frequency (iterations) - does NOT affect scheduler/optimizer
  log_frequency:
    train: 200    # Log train loss every N iterations
    val: null     # Log val metrics every N iterations (null = per epoch)

# Metrics configuration (same format as base aimnet2)
# Internal units: Energy (eV), Forces (eV/Angstrom), Charges (e)
# Scale factors convert eV to kcal/mol for display
metrics:
  class: aimnet2mult.train.metrics.RegMultiMetric
  kwargs:
    cfg:
      energy:
        abbr: E
        scale: 23.06  # eV to kcal/mol
      forces:
        abbr: F
        peratom: True
        mult: 3
        scale: 23.06   # eV/Å to kcal/mol/Å
      charges:
        abbr: q
        peratom: True
        scale: 1.0     # already in e
      spin_charges:
        abbr: s
        peratom: True
        scale: 1.0     # already in e
