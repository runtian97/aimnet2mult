# AIMNet2 Multi-Fidelity Training Configuration - OPTIMIZED
# Key changes from original:
# 1. Higher learning rate (5x increase) for faster convergence
# 2. Increased energy weight (5.0 vs 1.0) to prioritize energy accuracy
# 3. Less aggressive LR scheduler (patience 5, factor 0.8)
# 4. Added gradient clipping for stability
# 5. Increased epochs to 300 for better convergence

run_name: ???
project_name: ???

# Fidelity settings
fidelity_offset: 200
use_fidelity_readouts: true

data:
  fidelity_datasets: {}
  fidelity_weights: {}
  val_fidelity_datasets: null

  sae:
    energy:
      files: {}
      mode: linreg

  val_fraction: 0.05
  separate_val: true
  ddp_load_full_dataset: false

  x: [coord, numbers, charge, mult]
  y: [energy, forces, charges, spin_charges]

  samplers:
    train:
      kwargs:
        batch_size: 8192
        batch_mode: atoms
        shuffle: true
        batches_per_epoch: 20000
        sampling_strategy: weighted

    val:
      kwargs:
        batch_size: 4096
        batch_mode: atoms
        shuffle: false
        batches_per_epoch: -1
        sampling_strategy: uniform

  loaders:
    train:
      num_workers: 4
      pin_memory: true
    val:
      num_workers: 2
      pin_memory: true

# Loss function - OPTIMIZED: Increased energy weight for better energy RMSE
loss:
  class: aimnet2mult.train.loss.MTLoss
  kwargs:
    components:
      energy:
        fn: aimnet2mult.train.loss.energy_loss_fn
        weight: 2.0  # Increased from 1.0 - prioritize energy accuracy
      forces:
        fn: aimnet2mult.train.loss.peratom_loss_fn
        weight: 1.0  # Increased from 0.5 for balanced training
        kwargs:
          key_true: forces
          key_pred: forces
      charges:
        fn: aimnet2mult.train.loss.peratom_loss_fn
        weight: 0.05  # Slightly increased for charge accuracy
        kwargs:
          key_true: charges
          key_pred: charges
      spin_charges:
        fn: aimnet2mult.train.loss.peratom_loss_fn
        weight: 0.05  # Slightly increased for spin charge accuracy
        kwargs:
          key_true: spin_charges
          key_pred: spin_charges

# Optimizer - OPTIMIZED: Higher learning rate with gradient clipping
optimizer:
  force_no_train: []
  force_train: []
  class: torch.optim.AdamW
  kwargs:
    lr: 0.00025  # Increased 5x from 0.00005 for faster convergence
    weight_decay: 1e-5  # Slightly increased for regularization
    betas: [0.9, 0.999]
    eps: 1e-8
  param_groups:
    embeddings:
      re: 'afv.weight'
      lr: 0.000125  # Proportionally increased
      weight_decay: 0.0
    shifts:
      re: '.*.atomic_shift.shifts.weight'
      weight_decay: 0.0
  # Gradient clipping for training stability with higher LR
  grad_clip:
    max_norm: 1.0

# Learning rate scheduler - OPTIMIZED: Less aggressive decay
scheduler:
  class: ignite.handlers.param_scheduler.ReduceLROnPlateauScheduler
  kwargs:
    metric_name: loss
    factor: 0.8  # Less aggressive decay (was 0.9)
    patience: 5  # More patience before reducing LR (was 2)
  terminate_on_low_lr: 1.0e-7

# Training - OPTIMIZED: More epochs for convergence
trainer:
  epochs: 300  # Increased from 200

# Logging frequency (iterations)
log_frequency:
  train: 10
  val: 50

# Checkpointing - dirname will be overridden via CLI or defaults to ./checkpoints
checkpoint:
  dirname: checkpoints
  filename_prefix: ${run_name}
  save_best: true
  kwargs:
    n_saved: 5
    require_empty: false

# Weights & Biases
wandb:
  init:
    name: ${run_name}
    mode: online
    entity: null
    project: ${project_name}
    notes: null
  watch_model:
    log: gradients
    log_freq: 5000
    log_graph: false

# Metrics
metrics:
  class: aimnet2mult.train.metrics.RegMultiMetric
  kwargs:
    cfg:
      energy:
        abbr: E
        scale: 23.06
      forces:
        abbr: F
        peratom: true
        mult: 3
        scale: 23.06
      charges:
        abbr: q
        peratom: true
        scale: 1.0
      spin_charges:
        abbr: s
        peratom: true
        scale: 1.0
