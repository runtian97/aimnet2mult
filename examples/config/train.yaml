# AIMNet2 Multi-Fidelity Training Configuration

run_name: ???
project_name: ???

# Fidelity settings
fidelity_offset: 200
use_fidelity_readouts: true

data:
  fidelity_datasets: {}
  fidelity_weights: {}
  val_fidelity_datasets: null

  sae:
    energy:
      files: {}
      mode: linreg

  val_fraction: 0.005
  separate_val: true
  ddp_load_full_dataset: false

  x: [coord, numbers, charge, mult]
  y: [energy, forces, charges, spin_charges]

  samplers:
    train:
      kwargs:
        batch_size: 4096
        batch_mode: atoms
        shuffle: true
        batches_per_epoch: 20000
        sampling_strategy: weighted

    val:
      kwargs:
        batch_size: 4096
        batch_mode: atoms
        shuffle: false
        batches_per_epoch: -1
        sampling_strategy: uniform

  loaders:
    train:
      num_workers: 4
      pin_memory: true
    val:
      num_workers: 2
      pin_memory: true

# Loss function
loss:
  class: aimnet2mult.train.loss.MTLoss
  kwargs:
    components:
      energy:
        fn: aimnet2mult.train.loss.energy_loss_fn
        weight: 1.0
      forces:
        fn: aimnet2mult.train.loss.peratom_loss_fn
        weight: 0.5
        kwargs:
          key_true: forces
          key_pred: forces
      charges:
        fn: aimnet2mult.train.loss.peratom_loss_fn
        weight: 0.01
        kwargs:
          key_true: charges
          key_pred: charges
      spin_charges:
        fn: aimnet2mult.train.loss.peratom_loss_fn
        weight: 0.01
        kwargs:
          key_true: spin_charges
          key_pred: spin_charges

# Optimizer
optimizer:
  force_no_train: []
  force_train: []
  class: torch.optim.AdamW
  kwargs:
    lr: 0.00005
    weight_decay: 1e-6
    betas: [0.9, 0.999]
    eps: 1e-8
  param_groups:
    embeddings:
      re: 'afv.weight'
      lr: 0.000025
      weight_decay: 0.0
    shifts:
      re: '.*.atomic_shift.shifts.weight'
      weight_decay: 0.0

# Learning rate scheduler
scheduler:
  class: ignite.handlers.param_scheduler.ReduceLROnPlateauScheduler
  kwargs:
    metric_name: val_loss  # Use validation loss (recommended to avoid overfitting)
    factor: 0.9
    patience: 2
  terminate_on_low_lr: 1.0e-7

# Alternative: Ignite Cosine Annealing scheduler (uncomment to use)
# scheduler:
#   class: ignite.handlers.param_scheduler.CosineAnnealingScheduler
#   kwargs:
#     # param_name and optimizer are added automatically
#     start_value: 1.0e-4    # Starting learning rate
#     end_value: 1.0e-6      # Minimum learning rate at end of cycle
#     cycle_size: 200        # Number of iterations per cycle
#     cycle_mult: 1.0        # Multiply cycle_size by this after each cycle
#     start_value_mult: 1.0  # Multiply start_value after each cycle
#     end_value_mult: 1.0    # Multiply end_value after each cycle
#   terminate_on_low_lr: 0.0

# Training
trainer:
  epochs: 200

# Checkpointing - dirname will be overridden via CLI or defaults to ./checkpoints
checkpoint:
  dirname: checkpoints
  filename_prefix: ${run_name}
  save_best: true
  kwargs:
    n_saved: 5
    require_empty: false

# Weights & Biases
wandb:
  init:
    name: ${run_name}
    mode: online
    entity: null
    project: ${project_name}
    notes: null
  watch_model:
    log: all
    log_freq: 1000
    log_graph: true
  # Logging frequency (iterations) - does NOT affect scheduler/optimizer
  log_frequency:
    train: 500    # Log train loss every 500 iterations
         

log_frequency:
  val: 500 # Log val metrics every 500 iterations (null = per epoch)
# Metrics
metrics:
  class: aimnet2mult.train.metrics.RegMultiMetric
  kwargs:
    cfg:
      energy:
        abbr: E
        scale: 23.06
      forces:
        abbr: F
        peratom: true
        mult: 3
        scale: 23.06
      charges:
        abbr: q
        peratom: true
        scale: 1.0
      spin_charges:
        abbr: s
        peratom: true
        scale: 1.0
