# AIMNet2 Multi-Fidelity Training Configuration
# With Ignite CosineAnnealingScheduler (iteration-based)

run_name: ???
project_name: ???

# Fidelity settings
fidelity_offset: 200
use_fidelity_readouts: true

data:
  fidelity_datasets: {}
  fidelity_weights: {}
  val_fidelity_datasets: null

  sae:
    energy:
      files: {}
      mode: linreg

  val_fraction: 0.005
  separate_val: true
  ddp_load_full_dataset: false

  x: [coord, numbers, charge, mult]
  y: [energy, forces, charges, spin_charges]

  samplers:
    train:
      kwargs:
        batch_size: 4096
        batch_mode: atoms
        shuffle: true
        batches_per_epoch: 20000      # <-- Used to calculate cycle_size
        sampling_strategy: weighted

    val:
      kwargs:
        batch_size: 4096
        batch_mode: atoms
        shuffle: false
        batches_per_epoch: -1
        sampling_strategy: uniform

  loaders:
    train:
      num_workers: 4
      pin_memory: true
    val:
      num_workers: 2
      pin_memory: true

# Loss function
loss:
  class: aimnet2mult.train.loss.MTLoss
  kwargs:
    components:
      energy:
        fn: aimnet2mult.train.loss.energy_loss_fn
        weight: 1.0
      forces:
        fn: aimnet2mult.train.loss.peratom_loss_fn
        weight: 0.5
        kwargs:
          key_true: forces
          key_pred: forces
      charges:
        fn: aimnet2mult.train.loss.peratom_loss_fn
        weight: 0.01
        kwargs:
          key_true: charges
          key_pred: charges
      spin_charges:
        fn: aimnet2mult.train.loss.peratom_loss_fn
        weight: 0.01
        kwargs:
          key_true: spin_charges
          key_pred: spin_charges

# Optimizer
optimizer:
  force_no_train: []
  force_train: []
  class: torch.optim.AdamW
  kwargs:
    lr: 0.00005    # This will be overridden by scheduler start_value
    weight_decay: 1e-6
    betas: [0.9, 0.999]
    eps: 1e-8
  param_groups:
    embeddings:
      re: 'afv.weight'
      lr: 0.000025
      weight_decay: 0.0
    shifts:
      re: '.*.atomic_shift.shifts.weight'
      weight_decay: 0.0

# Learning rate scheduler - Ignite CosineAnnealingScheduler
scheduler:
  class: ignite.handlers.param_scheduler.CosineAnnealingScheduler
  kwargs:
    # param_name and optimizer are added automatically by get_scheduler()
    start_value: 1.0e-4        # Starting learning rate
    end_value: 1.0e-6          # Minimum learning rate at end of cycle
    cycle_size: 200000         # Number of iterations per cycle
                               # Calculation: 20000 batches/epoch Ã— 10 epochs = 200000
                               # Adjust based on your training duration
    cycle_mult: 1.0            # Keep cycle length constant (set > 1.0 for warm restarts)
    start_value_mult: 1.0      # Keep peak LR constant (set < 1.0 to decay over cycles)
    end_value_mult: 1.0        # Keep min LR constant
    warmup_duration: 0         # Optional: iterations of linear warmup before cosine
  terminate_on_low_lr: 0.0     # Stop training if LR hits exactly 0.0

# Training
trainer:
  epochs: 200

# Checkpointing - dirname will be overridden via CLI or defaults to ./checkpoints
checkpoint:
  dirname: checkpoints
  filename_prefix: ${run_name}
  save_best: true
  kwargs:
    n_saved: 5
    require_empty: false

# Weights & Biases
wandb:
  init:
    name: ${run_name}
    mode: online
    entity: null
    project: ${project_name}
    notes: null
  watch_model:
    log: all
    log_freq: 1000
    log_graph: true
  # Logging frequency (iterations)
  log_frequency:
    train: 500    # Log train loss every 500 iterations
         

log_frequency:
  val: 500  # Run validation and log metrics every 500 iterations

# Metrics
metrics:
  class: aimnet2mult.train.metrics.RegMultiMetric
  kwargs:
    cfg:
      energy:
        abbr: E
        scale: 23.06
      forces:
        abbr: F
        peratom: true
        mult: 3
        scale: 23.06
      charges:
        abbr: q
        peratom: true
        scale: 1.0
      spin_charges:
        abbr: s
        peratom: true
        scale: 1.0
