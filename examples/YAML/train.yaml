# AIMNet2 Multi-Fidelity Training Configuration
#
# Configure fidelity_datasets, fidelity_weights, and sae.energy.files
# via command line or by editing this file directly.

run_name: ???
project_name: ???

# Fidelity settings
fidelity_offset: 200
use_fidelity_readouts: true

data:
  # Fidelity datasets (specify paths or override via command line)
  fidelity_datasets: {}

  # Fidelity weights (higher weight = more samples from that fidelity)
  fidelity_weights: {}

  val_fidelity_datasets: null

  # SAE files (one per fidelity)
  sae:
    energy:
      files: {}
      mode: linreg

  # Validation settings
  val_fraction: 0.005
  separate_val: true
  ddp_load_full_dataset: false

  # Input and output keys
  x: [coord, numbers, charge, mult]
  y: [energy, forces, charges, spin_charges]

  # Sampler configuration
  samplers:
    train:
      kwargs:
        batch_size: 800
        batch_mode: atoms
        shuffle: true
        batches_per_epoch: 50000
        sampling_strategy: weighted

    val:
      kwargs:
        batch_size: 2000
        batch_mode: atoms
        shuffle: false
        batches_per_epoch: 1000
        sampling_strategy: uniform

  # DataLoader settings
  loaders:
    train:
      num_workers: 4
      pin_memory: true
    val:
      num_workers: 2
      pin_memory: true

# Loss function - energy-focused
loss:
  class: aimnet2mult.train.loss.MTLoss
  kwargs:
    components:
      energy:
        fn: aimnet2mult.train.loss.energy_loss_fn
        weight: 1.0
      forces:
        fn: aimnet2mult.train.loss.peratom_loss_fn
        weight: 0.05
        kwargs:
          key_true: forces
          key_pred: forces
      charges:
        fn: aimnet2mult.train.loss.peratom_loss_fn
        weight: 0.01
        kwargs:
          key_true: charges
          key_pred: charges
      spin_charges:
        fn: aimnet2mult.train.loss.peratom_loss_fn
        weight: 0.01
        kwargs:
          key_true: spin_charges
          key_pred: spin_charges

# Optimizer - AdamW
optimizer:
  force_no_train: []
  force_train: []
  class: torch.optim.AdamW
  kwargs:
    lr: 0.00005
    weight_decay: 1e-6
    betas: [0.9, 0.999]
    eps: 1e-8
  param_groups:
    embeddings:
      re: 'afv.weight'
      lr: 0.000025
      weight_decay: 0.0
    shifts:
      re: '.*.atomic_shift.shifts.weight$'
      weight_decay: 0.0

# Learning rate scheduler - ReduceLROnPlateau (adaptive)
scheduler:
  class: ignite.handlers.param_scheduler.ReduceLROnPlateauScheduler
  kwargs:
    metric_name: loss
    factor: 0.5
    patience: 5
  terminate_on_low_lr: 1.0e-7

# Training
trainer:
  epochs: 200

# Logging frequency (iterations)
log_frequency:
  train: 10
  val: 10

# Checkpointing
checkpoint:
  dirname: checkpoints
  filename_prefix: ${run_name}
  save_best: true             # Save best models by validation loss (not most recent)
  kwargs:
    n_saved: 10
    require_empty: false

# Weights & Biases logger
wandb:
  init:
    name: ${run_name}
    mode: online
    entity: null
    project: ${project_name}
    notes: null
  watch_model:
    log: gradients
    log_freq: 5000
    log_graph: false

# Metrics
metrics:
  class: aimnet2mult.train.metrics.RegMultiMetric
  kwargs:
    cfg:
      energy:
        abbr: E
        scale: 23.06
      forces:
        abbr: F
        peratom: true
        mult: 3
        scale: 23.06
      charges:
        abbr: q
        peratom: true
        scale: 1.0
      spin_charges:
        abbr: s
        peratom: true
        scale: 1.0
